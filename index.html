<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Complex Planning in Neuro-Symbolic Robot Manipulation">
  <meta name="keywords" content="NSRMP, NSRM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Complex Planning - Neuro-Symbolic Robot Manipulation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="static/images/favicon.svg" type="image/icon type">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Complex Planning in Neuro-Symbolic Robot Manipulation</h1>
          <h4 class="subtitle is-4 publication-subtitle"> Please read the group's previous work at <a href="https://nsrmp.github.io/">https://nsrmp.github.io/</a></h4>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered is-vcentered">
        <div class="column is-half">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/demo3_final.mp4"
                type="video/mp4">
      </video>
    </div>
    <div class="column is-one-third">
      <p class="has-text-centered">
        A demonstration on PyBullet: Constructing a house/shelter: The program library contains programs for rows, columns, towers and pyramids.
      <p>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Abstract</h2> -->
        <div class="content has-text-justified">
          <p>
          Given a natural language instruction, the goal is to build complex structures using the existing neuro-symbolic model through constraint solving in the simulator to output a manipulation program that can be executed by the robot on the input scene resulting in the desired output scene.
          </p>

          <iframe src="https://docs.google.com/presentation/d/e/2PACX-1vSq4WRzLmY-1oTPQxYT6KIvrfTob-E_3vsPLToEQmXbubfInnxeO_nHeefK45SaTvsgkSD3cXOrRfRd/embed?start=false&loop=false&delayms=60000" frameborder="0" width="800em" height="475em" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
          
          <!-- <p>Prior approaches for this task possess one of the following limitations: (i) rely on hand-coded symbols for concepts limiting generalization beyond those seen during training <sup><a href="ref1">[1]</a></sup> (ii) infer action sequences from instructions but require dense sub-goal supervision <sup><a href="ref2">[2]</a></sup> or (iii) lack semantics required for deeper object-centric reasoning inherent in interpreting complex instructions <sup><a href="#ref3">[3]</a></sup>. In contrast, our approach is neuro-symbolic and can handle linguistic as well as perceptual variations, is end-toend differentiable requiring no intermediate supervision, and makes use of symbolic reasoning constructs which operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene. 
          </p>
          
          <p>Central to our approach is a modular structure, consisting of a hierarchical instruction parser, and a manipulation module to learn disentangled action representations, both trained via RL. Our experiments on a simulated environment with a 7-DOF manipulator, consisting of instructions with varying number of steps, as well as scenes with different number of objects, and objects with unseen attribute combinations, demonstrate that our model is robust to such variations, and significantly outperforms existing baselines, particularly in generalization settings
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

<center>
  <p>Harsh Vora</p>
  <p>Check out my other projects: <a href="https://github.com/harshvora64">
    github <i class="fab fa-github"></i></a></p>
  <p> Also check out my <a href="https://www.linkedin.com/in/harsh-vora-684a54224">
    LinkedIn profile <i class="fab fa-linkedin"></i> </a></p>
    <br
</center>

<footer class="footer">

  <div class="container">
    <div class="content has-text-centered">Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a></div>
  </div>
</footer>

</body>
</html>
